{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "到最后了还是忍不住想用ResNet试一下，因为ResNet给人的美感太强了，虽然自己已经交作业了，不过还是很想看看ResNet的效果。这个notebook估计自己想这样做，先自己从头训练一下看看效果，再用预训练的模型试一下。\n",
    "\n",
    "## 从头开始的ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义残差块\n",
    "\n",
    "这里借鉴了李沐老师的动手学习深度学习的代码，架构如下：\n",
    "\n",
    "![resnet](resnet-block.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):  #@save\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 残差块小测试\n",
    "\n",
    "我们测试一下输入输出的维度是怎么样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((4, 3, 6, 6))\n",
    "resBlock = Residual(3, 3)\n",
    "resBlock(X).shape # torch.Size([4, 3, 6, 6]) # same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resBlock = Residual(3, 6, use_1x1conv=True, strides=2)\n",
    "resBlock(X).shape # torch.Size([4, 6, 3, 3]) # 1/2 shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "复习一下`nn.Conv2d`的输入输出大小计算公式\n",
    "\n",
    "$$\n",
    "\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n",
    "        \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
    "                        \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor \n",
    "$$\n",
    "\n",
    "$$\n",
    "W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
    "                        \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv2d(3, 3, kernel_size=3, padding=1, stride=1)(X).shape # torch.Size([4, 3, 6, 6]) # same shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv2d(3, 3, kernel_size=3, padding=1, stride=2)(X).shape # torch.Size([4, 3, 3, 3]) # 1/2 shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义ResNet的block\n",
    "\n",
    "ResNet在第一个block的时候是和谷歌的Inception是一样的，不同的是ResNet在卷积层之后加了一个BatchNorm层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "block1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "                          nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 56, 56])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((4, 3, 224, 224))\n",
    "block1(X).shape # torch.Size([4, 64, 56, 56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义除了block1之外的block网络结构\n",
    "def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                 first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels,\n",
    "                                use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接着在ResNet加入所有残差块，这里每个模块使用2个残差块。这样我们下面每个模块就有4个卷积层，下面就一共16个卷积层\n",
    "block2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "block3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "block4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "block5 = nn.Sequential(*resnet_block(256, 512, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 残差block小测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 56, 56])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((4, 64, 56, 56))\n",
    "block2(X).shape # torch.Size([4, 64, 56, 56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 28, 28])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block3(X).shape # torch.Size([4, 128, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 14, 14])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((4, 128, 28, 28))\n",
    "block4(X).shape # torch.Size([4, 256, 14, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 7, 7])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((4, 256, 14, 14))\n",
    "block5(X).shape # torch.Size([4, 512, 7, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((4, 512, 7, 7))\n",
    "nn.AdaptiveAvgPool2d((1, 1))(X).shape # torch.Size([4, 512, 1, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet18 = nn.Sequential(block1, block2, block3, block4, block5,\n",
    "                          nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                            nn.Flatten(),\n",
    "                            nn.Linear(512, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 128, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 256, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 3, 224, 224))\n",
    "for layer in ResNet18:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 64, 56, 56]           --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 112, 112]         9,472\n",
       "│    └─BatchNorm2d: 2-2                  [1, 64, 112, 112]         128\n",
       "│    └─ReLU: 2-3                         [1, 64, 112, 112]         --\n",
       "│    └─MaxPool2d: 2-4                    [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-2                        [1, 64, 56, 56]           --\n",
       "│    └─Residual: 2-5                     [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           36,928\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n",
       "│    │    └─Conv2d: 3-3                  [1, 64, 56, 56]           36,928\n",
       "│    │    └─BatchNorm2d: 3-4             [1, 64, 56, 56]           128\n",
       "│    └─Residual: 2-6                     [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-5                  [1, 64, 56, 56]           36,928\n",
       "│    │    └─BatchNorm2d: 3-6             [1, 64, 56, 56]           128\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 56, 56]           36,928\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           128\n",
       "├─Sequential: 1-3                        [1, 128, 28, 28]          --\n",
       "│    └─Residual: 2-7                     [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-9                  [1, 128, 28, 28]          73,856\n",
       "│    │    └─BatchNorm2d: 3-10            [1, 128, 28, 28]          256\n",
       "│    │    └─Conv2d: 3-11                 [1, 128, 28, 28]          147,584\n",
       "│    │    └─BatchNorm2d: 3-12            [1, 128, 28, 28]          256\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 28, 28]          8,320\n",
       "│    └─Residual: 2-8                     [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-14                 [1, 128, 28, 28]          147,584\n",
       "│    │    └─BatchNorm2d: 3-15            [1, 128, 28, 28]          256\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 28, 28]          147,584\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 28, 28]          256\n",
       "├─Sequential: 1-4                        [1, 256, 14, 14]          --\n",
       "│    └─Residual: 2-9                     [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-18                 [1, 256, 14, 14]          295,168\n",
       "│    │    └─BatchNorm2d: 3-19            [1, 256, 14, 14]          512\n",
       "│    │    └─Conv2d: 3-20                 [1, 256, 14, 14]          590,080\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 256, 14, 14]          512\n",
       "│    │    └─Conv2d: 3-22                 [1, 256, 14, 14]          33,024\n",
       "│    └─Residual: 2-10                    [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-23                 [1, 256, 14, 14]          590,080\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 256, 14, 14]          512\n",
       "│    │    └─Conv2d: 3-25                 [1, 256, 14, 14]          590,080\n",
       "│    │    └─BatchNorm2d: 3-26            [1, 256, 14, 14]          512\n",
       "├─Sequential: 1-5                        [1, 512, 7, 7]            --\n",
       "│    └─Residual: 2-11                    [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-27                 [1, 512, 7, 7]            1,180,160\n",
       "│    │    └─BatchNorm2d: 3-28            [1, 512, 7, 7]            1,024\n",
       "│    │    └─Conv2d: 3-29                 [1, 512, 7, 7]            2,359,808\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 512, 7, 7]            1,024\n",
       "│    │    └─Conv2d: 3-31                 [1, 512, 7, 7]            131,584\n",
       "│    └─Residual: 2-12                    [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-32                 [1, 512, 7, 7]            2,359,808\n",
       "│    │    └─BatchNorm2d: 3-33            [1, 512, 7, 7]            1,024\n",
       "│    │    └─Conv2d: 3-34                 [1, 512, 7, 7]            2,359,808\n",
       "│    │    └─BatchNorm2d: 3-35            [1, 512, 7, 7]            1,024\n",
       "├─AdaptiveAvgPool2d: 1-6                 [1, 512, 1, 1]            --\n",
       "├─Flatten: 1-7                           [1, 512]                  --\n",
       "├─Linear: 1-8                            [1, 10]                   5,130\n",
       "==========================================================================================\n",
       "Total params: 11,184,650\n",
       "Trainable params: 11,184,650\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.82\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 38.33\n",
       "Params size (MB): 44.74\n",
       "Estimated Total Size (MB): 83.68\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(ResNet18, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "num_epochs = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集及可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 定义 transform，包括缩放、中心裁剪、随机水平翻转、归一化\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 只需要归一化和中心裁剪\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# 加载 CIFAR10 数据集\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform_test)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "### 下面训练和验证的代码部分参考了微软家的pytorch教程\n",
    "# https://learn.microsoft.com/en-us/training/modules/intro-computer-vision-pytorch/6-transfer-learning\n",
    "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = nn.NLLLoss()):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    net.train()\n",
    "    total_loss,acc,count = 0,0,0\n",
    "    for features,labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        lbls = labels.to(device)\n",
    "        out = net(features.to(device))\n",
    "        loss = loss_fn(out,lbls) #cross_entropy(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss\n",
    "        _,predicted = torch.max(out,1)\n",
    "        acc+=(predicted==lbls).sum()\n",
    "        count+=len(labels)\n",
    "    return total_loss.item()/count, acc.item()/count\n",
    "\n",
    "def validate(net, dataloader,loss_fn=nn.NLLLoss()):\n",
    "    net.eval()\n",
    "    count,acc,loss = 0,0,0\n",
    "    with torch.no_grad():\n",
    "        for features,labels in dataloader:\n",
    "            lbls = labels.to(device)\n",
    "            out = net(features.to(device))\n",
    "            loss += loss_fn(out,lbls) \n",
    "            pred = torch.max(out,1)[1]\n",
    "            acc += (pred==lbls).sum()\n",
    "            count += len(labels)\n",
    "    return loss.item()/count, acc.item()/count\n",
    "\n",
    "def train(net,train_loader,test_loader,optimizer=None,lr=0.01,epochs=10,loss_fn=nn.NLLLoss()):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    res = { 'train_loss' : [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    for ep in range(epochs):\n",
    "        tl,ta = train_epoch(net,train_loader,optimizer=optimizer,lr=lr,loss_fn=loss_fn)\n",
    "        vl,va = validate(net,test_loader,loss_fn=loss_fn)\n",
    "        print(f\"Epoch {ep:2}, Train acc={ta:.3f}, Val acc={va:.3f}, Train loss={tl:.3f}, Val loss={vl:.3f}\")\n",
    "        res['train_loss'].append(tl)\n",
    "        res['train_acc'].append(ta)\n",
    "        res['val_loss'].append(vl)\n",
    "        res['val_acc'].append(va)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESNET18 = ResNet18.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() # 使用交叉熵损失函数\n",
    "optimizer = torch.optim.Adam(ResNet18.parameters(), lr=learning_rate) # 使用 Adam 优化器\n",
    "\n",
    "# 训练模型\n",
    "history = train(ResNet18, trainloader, testloader, optimizer, num_epochs, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_long(net,train_loader,test_loader,epochs=5,lr=0.01,optimizer=None,loss_fn = nn.NLLLoss(),print_freq=10):\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        total_loss,acc,count = 0,0,0\n",
    "        for i, (features,labels) in enumerate(train_loader):\n",
    "            lbls = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = net(features.to(device))\n",
    "            loss = loss_fn(out,lbls)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss+=loss\n",
    "            _,predicted = torch.max(out,1)\n",
    "            acc+=(predicted==lbls).sum()\n",
    "            count+=len(labels)\n",
    "            if i%print_freq==0:\n",
    "                print(\"Epoch {}, minibatch {}: train acc = {}, train loss = {}\".format(epoch,i,acc.item()/count,total_loss.item()/count))\n",
    "        vl,va = validate(net,test_loader,loss_fn)\n",
    "        print(\"Epoch {} done, validation acc = {}, validation loss = {}\".format(epoch,va,vl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,shuffle=False, num_workers=2)\n",
    "\n",
    "train_long(net,trainloader,testloader,loss_fn=torch.nn.CrossEntropyLoss(),epochs=2,print_freq=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
